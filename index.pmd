% KNN Project
% Jon Craton
% November 9th, 2018

# Anaconda Installation

Following the instructions available here Anaconda can be installed on my system:

http://docs.continuum.io/anaconda/install/linux/

I walked through this and have installed Anaconda several times.

# Initialization

I've restructured much of the code from the original IPython notebook here.

I've also chosen to use [pweave](http://mpastell.com/pweave/) to generate my final output. pweave uses IPython to execute chunks of code found in a text file. This allows me to execute the code from the notebook in a repeatable way that can also be included in a clean final PDF output.

Let's pull in the initialization code. I've also included Seaborn, as I think the default matplotlib plots don't look great.

```python
from sklearn.datasets.samples_generator import make_blobs
import numpy as np
import matplotlib.pyplot as plt

import seaborn as sns
sns.set(style="darkgrid")
sns.set_context("paper")

np.random.seed(6)
import math
from collections import Counter
```

# Data Generation

I've created a function out of the example code to generate multiple scatter plots easily:

```python
def gen_data(n, cluster_std, plot=True, features=2):
  n_training = int(.9*n)

  (X,y) =  make_blobs(n_samples=n,n_features=features,centers=2,cluster_std=cluster_std,random_state=50)

  if plot:
    colors = ['tab:red','tab:blue','tab:green','tab:orange']
    c = [colors[c] for c in np.concatenate((y[:n_training], np.add(2,y[n_training:])))]
    sns.set_palette("husl")
    plt.scatter(X[:,0],X[:,1],c=c)
    plt.title("n=%d, std=%.02f" % (n, cluster_std))
    plt.show()

  return (
          X[:n_training],
          y[:n_training],
          X[n_training:],
          y[n_training:]
         )
```

Here's the original data from the sample notebook:

```python
(
  X_train, y_train, 
  X_val, y_val
) = gen_data(100, 1.95)
```

# Data Plots

Let's use this to create the requested plots:

```python
for std in [.5,1,1.5]:
  for n in [50,100,150]:
    gen_data(n, std)
```

# Prediction Code

```python
def get_knn(X,point,k,distance='euclidean'):
  """ 
  Returns the indices of `k` nearest neighbors to `point` in `X`

  >>> get_knn(np.array([(1,1)]),(1,1),1)
  array([0])
  >>> get_knn(np.array([(1,2),(1,1)]),(1,1),1)
  array([1])
  >>> get_knn(np.array([(1,4),(1,1),(1,3),(1,1.5)]),(1,1),2)
  array([1, 3])
  >>> get_knn(np.array([(1,4),(1,1),(1,3),(1,1.5)]),(1,1),2,distance='manhattan')
  array([1, 3])
  >>> get_knn(np.array([(1,1),(6,6),(1,10)]),(1,1),2,distance='manhattan')
  array([0, 2])
  """
  if distance == 'euclidean':
    distances = np.sqrt(np.sum((X - point)**2, axis=1))
  else:
    distances = np.sum(np.abs(X - point), axis=1)
  return np.argsort(distances)[0:k]

def predict(prediction_points,k,distance='euclidean'):
  points_labels=[]
  
  for point in prediction_points:
    results=[]
    for index in get_knn(X_train,point,k,distance=distance):
      results.append(y_train[index])
    
    label=Counter(results).most_common(1)
    points_labels.append([point,label[0][0]])
      
  return points_labels

def get_accuracy(predictions):
  error=np.sum((predictions-y_val)**2)
  accuracy=100-(error/len(y_val))*100
  return accuracy

def plot_accuracy(dist,k=20):
  acc=[]
  for k in range(1,k+1,int(k/20)):
    predictions=[r[1] for r in predict(X_val,k,distance=dist)]
    acc.append([get_accuracy(predictions),k])
  
  plt.plot([a[1] for a in acc], [a[0] for a in acc], label=dist)
  plt.legend()
```

# Prediction Accuracy

```python
for std in [.5,1,1.5,3.0]:
  for n in [50,100,150,500]:
    (
      X_train, y_train, 
      X_val, y_val
    ) = gen_data(n,std)

    plot_accuracy('euclidean')
    plot_accuracy('manhattan')
    plt.xlabel("Neighbors")
    plt.ylabel("Accuracy")
    plt.show()
```

# Conclusions

The above plots show us the following:

1. Tightly clustered data is very easy to predict with perfect accuracy using KNN, even by simply looking at one nearest neighbor.
2. Larger K is generally better.
  - This wasn't quite true on the n=100 and std=1.50 plot due to some tricky validation points, but it is a very clear on the more challenging classifications (Ïƒ=3.0).
3. Euclidean and Manhattan distance largely perform the same function for this data. It appeared to basically be a wash between the two.

# Dimensionality

These conclusions are somewhat unsatisfying, so let's try increasing our number of features to explore the real difference between these distance algorithms.

```python
for std in [10.0,20.0,50.0]:
  (
    X_train, y_train, 
    X_val, y_val
  ) = gen_data(100,std,plot=False,features=20)

  plot_accuracy('euclidean',k=40)
  plot_accuracy('manhattan',k=40)
  plt.xlabel("Neighbors")
  plt.ylabel("Accuracy")
  plt.show()
```
