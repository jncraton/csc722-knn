% KNN Project
% Jon Craton
% November 9th, 2018

1. Anaconda Installation
========================

> Install Anaconda from (https://www.anaconda.com/download/) and run the code that I have provided. Installing Anaconda and being able to work with it is part of project.  

Following the [instructions](http://docs.continuum.io/anaconda/install/linux/) Anaconda can be installed on my system. I walked through this and have installed Anaconda several times.

![](media/anaconda-install-start.png)

…

![](media/anaconda-install-complete.png)

2. Plots 
========

> Please plot the training points in section (a) for the following sets and explain your observation. If you could not save the figures from Anaconda, you can use PrintScreen  to add the plots/figures to your report but I would recommend finding the command line that you can save the figures from Anaconda. 

Initialization
--------------

I've restructured much of the code from the original IPython notebook here.

I've chosen to use [pweave](http://mpastell.com/pweave/) to generate my final output. pweave uses IPython to execute chunks of code found in a text file. This allows me to execute the code from the notebook in a repeatable way that can also be included in a clean final PDF output.

Let's pull in our imports and set a standard seed for the RNG.

```python
from sklearn.datasets.samples_generator import make_blobs
import numpy as np
np.random.seed(6)

from collections import Counter

import matplotlib.pyplot as plt
```

Let's also include Seaborn to make things look a little nicer.

```python
import seaborn as sns
sns.set(style="darkgrid")
sns.set_context("paper")
```

Generation
----------

I've created a function out of the example code to generate multiple scatter plots easily:

```python
def gen_data(n, cluster_std, plot=True, features=2):
  n_training = int(.9*n)

  (X,y) =  make_blobs(n_samples=n,n_features=features,centers=2,cluster_std=cluster_std,random_state=50)

  if plot:
    colors = ['tab:red','tab:blue','tab:green','tab:orange']
    c = [colors[c] for c in np.concatenate((y[:n_training], np.add(2,y[n_training:])))]
    sns.set_palette("husl")
    plt.scatter(X[:,0],X[:,1],c=c)
    plt.title("n=%d, σ=%.01f" % (n, cluster_std))
    plt.show()

  return (
          X[:n_training],
          y[:n_training],
          X[n_training:],
          y[n_training:]
         )
```

Here's the original data from the sample notebook:

```python
(
  X_train, y_train, 
  X_val, y_val
) = gen_data(100, 1.95)
```

Plots
-----

Let's use this to create the requested plots:

```python
for std in [.5,1,1.5]:
  for n in [50,100,150]:
    gen_data(n, std)
```

3. Manhattan Distance
=====================

> Modify the code in section (c) to calculate the Manhattan distance.

Here's our orginal distance function:

```python
def get_eculidean_distance(point,k):
  euc_distance = np.sqrt(np.sum((X_train - point)**2 , axis=1))
  return np.argsort(euc_distance)[0:k]
```

Let's create a new function that can take distance a parameter and return k nearest neighbors using either Euclidean or Manhattan distance. We'll use `np.sum(np.abs(X - point), axis=1)` to calculate Manhattan distance.

```python
def get_knn(X,point,k,distance='euclidean'):
  """ 
  Returns the indices of `k` nearest neighbors to `point` in `X`

  >>> get_knn(np.array([(1,1)]),(1,1),1)
  array([0])
  >>> get_knn(np.array([(1,2),(1,1)]),(1,1),1)
  array([1])
  >>> get_knn(np.array([(1,4),(1,1),(1,3),(1,1.5)]),(1,1),2)
  array([1, 3])
  >>> get_knn(np.array([(1,4),(1,1),(1,3),(1,1.5)]),(1,1),2,distance='manhattan')
  array([1, 3])
  >>> get_knn(np.array([(1,1),(6,6),(1,10)]),(1,1),2,distance='manhattan')
  array([0, 2])
  """
  if distance == 'euclidean':
    distances = np.sqrt(np.sum((X - point)**2, axis=1))
  else:
    distances = np.sum(np.abs(X - point), axis=1)
  return np.argsort(distances)[0:k]
```

4. Impact of k
==============

> Change K from 1 – 20 and plot the accuracy for both Euclidian and Manhattan distances. Explain what you have observed. 

Prediction
----------

First, we'll need some code to return actual predictions. The prediction returned by KNN is simply the most common class of the k nearest neighbors.

```python
def predict(prediction_points,k,distance='euclidean'):
  points_labels=[]
  
  for point in prediction_points:
    results=[]
    for index in get_knn(X_train,point,k,distance=distance):
      results.append(y_train[index])
    
    label=Counter(results).most_common(1)
    points_labels.append([point,label[0][0]])
      
  return points_labels
```

Next, we'll add a measure of accuracy. This is simply the number of correct predictions over the number of attempts.

```python
def get_accuracy(predictions):
  error=np.sum((predictions-y_val)**2)
  accuracy=100-(error/len(y_val))*100
  return accuracy
```

Sweeping k
----------

Let's now create a function to sweep through K values and plot accuracy against them.

```python
def plot_accuracy(dist,k_max=20):
  acc=[]
  for k in range(1,k_max+1,int(k_max/20)):
    predictions=[r[1] for r in predict(X_val,k,distance=dist)]
    acc.append([get_accuracy(predictions),k])
  
  plt.plot([a[1] for a in acc], [a[0] for a in acc], label=dist)
  plt.legend()

def plot_comparative_accuracy(n,std,k_max=20):
  plot_accuracy('euclidean',k_max=k_max)
  plot_accuracy('manhattan',k_max=k_max)
  plt.xlabel("Neighbors")
  plt.ylabel("Accuracy")
  plt.title("Accuracy of n=%d and σ=%.01f" % (n,std))
  plt.show()
```

Now let's sweep through different values of K on our different data sets and explore the results.

```python
for std in [.5,1,1.5]:
  for n in [50,100,150]:
    (
      X_train, y_train, 
      X_val, y_val
    ) = gen_data(n,std)

    plot_comparative_accuracy(n,std)
```

I'm not entirely satisfied with those results, so let's add some samples with larger n and larger σ to see if this provides more interesting accuracy.

```python
for std in [3.0]:
  for n in [150,500]:
    (
      X_train, y_train, 
      X_val, y_val
    ) = gen_data(n,std)

    plot_comparative_accuracy(n,std)
```

Conclusions
-----------

The above plots show us the following:

1. Tightly clustered data is very easy to predict with perfect accuracy using KNN, even by simply looking at one nearest neighbor.
2. Larger K is generally better.
3. Euclidean and Manhattan distance largely perform the same function for this data. It appeared to basically be a wash between the two.
4. As expected, the more overlap we see between datasets, the harder it is to accurately classify data.

Additional Exploration
======================

I was a little disappointed to find no real difference between Manhattan and Euclidean distance. 

I did a little [additional searching](https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions), and found that differences between these distance functions may show up in [higher dimensions](https://en.wikipedia.org/wiki/Curse_of_dimensionality#Distance_functions).
Let's try generating some higher-dimensional data and see if we can find a difference between the distance formulas.

I'll create a custom data set consisting of 200 variables. The data will be split into two classes. Only the first 2 variables will be related to the class, with the remaining variables being random noise. We'll then plot the comparative accuracy of our two distance functions.

```python
from random import random

n = 500
f = 200
std = .5

for run in range(0,3): # Run a few times for consistency
  y = [int(i % 2) for i in range(0,n)]

  X = [[2*random()-1 + (2*y[i]-1),2*random()-1 + (2*y[i]-1)] + [5*(random()-.5) for j in range(0,f-2)] for i in range(0,n)]

  X_train = np.array(X[:int(f*.9)])
  y_train = np.array(y[:int(f*.9)])
  X_val = np.array(X[int(f*.9):])
  y_val = np.array(y[int(f*.9):])

  plot_comparative_accuracy(n,std,k_max=100)
```

We can see that in this higher-dimensional dataset, Manhattan distance works out much better for us for all values of k.

